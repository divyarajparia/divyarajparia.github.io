<!-- projects.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Projects - Divya Rajparia</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <nav>
    <ul style="list-style: none; display: flex; gap: 20px; padding: 10px; background-color: #f2f2f2;">
      <li><a href="index.html">Home</a></li>
      <li><a href="resume.html">Resume</a></li>
      <li><a href="projects.html" class="active">Projects</a></li>
      <li><a href="timeline.html">Timeline</a></li>
      <li><a href="extracurriculars.html">Extracurriculars</a></li>
      <li><a href="contact.html">Contact</a></li>
    </ul>
  </nav>

  <main>
    <h1>Projects</h1>

    <section>
      <h2>Enhancing Tumor Segmentation using Synthetic Data</h2>
      <p>
        I am currently working as a research intern at the Laboratory for Machine Learning, Health, and Biomedicine at the University of Southern California. The project focuses on improving and automating the challenging task of tumor segmentation in breast and lung cancer.
      </p>
      <p>
        Manual annotation of medical images is time-consuming and requires significant effort from clinicians. Our goal is to reduce this burden by developing a segmentation model that is both accurate and generalizable. A major challenge is the domain shift that occurs when models trained on one dataset are applied to another.
      </p>
      <p>
        We address this by generating synthetic data to simulate a variety of domain scenarios. This helps the model learn more robust representations. The ultimate aim is to build a segmentation pipeline that can handle diverse real-world inputs and improve productivity in clinical workflows.
      </p>
    </section>

    <section>
      <h2>DES: Dynamic Expert Saturation (Continual Learning)</h2>
      <p>
        This project was conducted at IIT Hyderabad as part of my research in Continual Learning, a paradigm where the goal is to train models on tasks that arrive over time—like how humans learn—without forgetting past knowledge.
      </p>
      <p>
        Most existing methods rely on unrealistic assumptions like having access to balanced data or stored old samples, which is rarely possible in practical settings. We proposed a new learning setup called AFCIL (Assumption-Free Class-Incremental Learning) that removes these assumptions. It does not require stored data, handles imbalanced tasks, and supports pre-trained models like CLIP.
      </p>

      <div style="text-align: center; margin: 20px 0;">
        <img src="CL1.jpg" alt="CL1" style="width: 240px; border-radius: 10px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);" />
        <p style="font-size: 0.9em; color: #555;">Figure 1: Our Problem Setting</p>
      </div>

      <p>
        We introduced a two-stage training strategy called Dynamic Expert Saturation. In the first stage, the model is trained on head (frequent) classes. In the second stage, it incrementally saturates the model to tail (rare) classes using dynamically added lightweight expert adapters. This naturally handles class imbalance and avoids catastrophic forgetting.
      </p>

      <div style="text-align: center; margin: 20px 0;">
        <img src="CL2.jpg" alt="CL2" style="width: 240px; border-radius: 10px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);" />
        <p style="font-size: 0.9em; color: #555;">Figure 2: Our Model Architecture and Trainnig Phases</p>
      </div>

      <p>
        The model was implemented using PyTorch with a CLIP backbone and lightweight MLP adapters. We achieved state-of-the-art performance on CIFAR100-LT and ImageNet-LT, outperforming prior methods such as DualPrompt, CODA, and MoE-Adapter by up to 6% in top-1 accuracy.
      </p>
      <p>
        <a href="Divya_Poster Final.pdf" target="_blank">Poster for this Project</a>
      </p>
    </section>

    <section>
      <h2>Making Vision Transformers More Human-Like: Exploring Compositionality</h2>
      <p>
        This research project was conducted at the Lab for Vision and Image Analysis, IIT Hyderabad, and has been submitted to NeurIPS 2025. We investigated how Vision Transformers (ViTs) understand complex visual inputs by studying their ability to combine parts into whole representations.
      </p>
      <p>
        We designed a framework using wavelet transforms to decompose images into frequency-based primitives. We then tested whether ViTs could reconstruct meaningful representations by linearly combining these parts. The models were built and trained in PyTorch and evaluated on the ImageNet dataset.
      </p>

      <div style="text-align: center; margin: 20px 0;">
        <img src="ViT1.jpg" alt="ViT1" style="width: 240px; border-radius: 10px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);" />
        <p style="font-size: 0.9em; color: #555;">Figure 3: Framework of out Wavelet-based image decomposition and re-composition</p>
      </div>

      <p>
        Our experiments demonstrated that ViTs—despite being highly nonlinear—exhibit compositional behavior in deeper layers. We also found that this structure persists under real-world distortions such as additive noise and JPEG compression, suggesting that these properties are both meaningful and robust.
      </p>

      <div style="text-align: center; margin: 20px 0;">
        <img src="ViT2.jpg" alt="ViT2" style="width: 240px; border-radius: 10px; box-shadow: 0 2px 5px rgba(0,0,0,0.2);" />
        <p style="font-size: 0.9em; color: #555;">Figure 4: Reconstructing Images in Pixel Space using weghts learned on Representation Space</p>
      </div>

      <p>
        <a href="Down_the_Rabbit_Hole_of_Vision_Transformers.pdf" target="_blank">Paper Submission for this Project</a>
      </p>
    </section>

  </main>
</body>
</html>
